# ğŸ½ï¸ TopGo RAG Chatbot

Há»‡ thá»‘ng gá»£i Ã½ nhÃ  hÃ ng thÃ´ng minh sá»­ dá»¥ng RAG (Retrieval-Augmented Generation) vá»›i **Ollama Local LLM** - Miá»…n phÃ­, khÃ´ng giá»›i háº¡n.

---

## âœ¨ TÃ­nh nÄƒng

- ğŸ¤– **Chat AI vá»›i Ollama** - LLM cháº¡y local, 100% miá»…n phÃ­
- ğŸ” **Semantic Search** - TÃ¬m kiáº¿m thÃ´ng minh vá»›i embeddings
- ğŸ¯ **Gá»£i Ã½ cÃ¡ nhÃ¢n hÃ³a** - Dá»±a trÃªn ngá»¯ cáº£nh vÃ  sá»Ÿ thÃ­ch
- ğŸ“Š **1891+ nhÃ  hÃ ng** - Dá»¯ liá»‡u crawl tá»« TopGo.vn (HÃ  Ná»™i)
- ğŸŒ **FastAPI Backend** + ğŸ¨ **Streamlit UI**
- ğŸ”’ **100% Local** - KhÃ´ng cáº§n API keys, báº£o máº­t tuyá»‡t Ä‘á»‘i

---

## ğŸš€ Quick Start

### BÆ°á»›c 1: CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### BÆ°á»›c 2: Setup Ollama (LLM Local)

```bash
# Download vÃ  cÃ i Ä‘áº·t Ollama: https://ollama.ai
# Windows: Cháº¡y OllamaSetup.exe
# Mac: brew install ollama
# Linux: curl -fsSL https://ollama.ai/install.sh | sh

# Pull model (chá»n 1 trong cÃ¡c model sau)
ollama pull qwen2:1.5b      # Model nháº¹, nhanh (1.5GB) â­ Khuyáº¿n nghá»‹
ollama pull llama2          # Model lá»›n hÆ¡n (4.7GB)
ollama pull vinallama       # Model tiáº¿ng Viá»‡t (2GB)
```

ğŸ“– **HÆ°á»›ng dáº«n chi tiáº¿t:** [docs/SETUP_OLLAMA.md](docs/SETUP_OLLAMA.md)

### BÆ°á»›c 3: Cháº¡y á»©ng dá»¥ng

```bash
# CÃ¡ch 1: DÃ¹ng script (Windows)
.\start_all.bat

# CÃ¡ch 2: Cháº¡y trá»±c tiáº¿p
streamlit run app.py
```

Má»Ÿ trÃ¬nh duyá»‡t: **http://localhost:8501**

**LÆ°u Ã½:** 
- âœ… App cháº¡y Ä‘Æ°á»£c ngay cáº£ khi Ollama chÆ°a cÃ³ (cháº¿ Ä‘á»™ search-only)
- ğŸ¤– Äá»ƒ dÃ¹ng AI chat, cáº§n khá»Ÿi Ä‘á»™ng Ollama: `ollama serve`
- ğŸ“Š API backend (optional): `python run_api.py`

---

## ğŸ“ Cáº¥u trÃºc Project

```
topgo-rag-chatbot/
â”œâ”€â”€ data/                      # ğŸ“‚ Dá»¯ liá»‡u
â”‚   â”œâ”€â”€ raw/                   # Dá»¯ liá»‡u thÃ´ crawl Ä‘Æ°á»£c
â”‚   â”œâ”€â”€ processed/             # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ (JSON)
â”‚   â””â”€â”€ vector_db/             # ChromaDB vector database
â”‚
â”œâ”€â”€ src/                       # ğŸ”§ Source code chÃ­nh
â”‚   â”œâ”€â”€ api/                   # FastAPI REST API
â”‚   â”œâ”€â”€ crawlers/              # Web scraping TopGo.vn
â”‚   â”œâ”€â”€ embeddings/            # Vector embeddings & search
â”‚   â”œâ”€â”€ llm/                   # Ollama LLM client
â”‚   â””â”€â”€ rag/                   # RAG pipeline & prompts
â”‚
â”œâ”€â”€ docs/                      # ğŸ“š TÃ i liá»‡u
â”‚   â”œâ”€â”€ DOCUMENTATION.md       # TÃ i liá»‡u Ä‘áº§y Ä‘á»§
â”‚   â”œâ”€â”€ SETUP_OLLAMA.md        # HÆ°á»›ng dáº«n cÃ i Ollama
â”‚   â””â”€â”€ ...                    # CÃ¡c tÃ i liá»‡u khÃ¡c
â”‚
â”œâ”€â”€ scripts/                   # ğŸ› ï¸ Utility scripts
â”‚   â”œâ”€â”€ setup_ollama.bat       # Setup Ollama tá»± Ä‘á»™ng
â”‚   â””â”€â”€ rebuild_embeddings.py  # Rebuild vector DB
â”‚
â”œâ”€â”€ app.py                     # ğŸ¨ Streamlit UI (Main App)
â”œâ”€â”€ start_all.bat              # ğŸš€ Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
â”œâ”€â”€ stop_all.bat               # ğŸ›‘ Dá»«ng táº¥t cáº£ services
â”œâ”€â”€ requirements.txt           # ğŸ“¦ Python dependencies
â”œâ”€â”€ .env                       # âš™ï¸ Environment config
â””â”€â”€ README.md                  # ğŸ“– Báº¡n Ä‘ang Ä‘á»c file nÃ y
```

---

## ğŸ¯ Sá»­ dá»¥ng

### 1. Khá»Ÿi Ä‘á»™ng nhanh

```bash
# Windows: Double-click hoáº·c command line
.\start_all.bat

# Hoáº·c cháº¡y trá»±c tiáº¿p
streamlit run app.py
```

### 2. Chat vá»›i AI

```
ğŸ‘¤ Báº¡n: "TÃ¬m nhÃ  hÃ ng Viá»‡t Nam bÃ¬nh dÃ¢n á»Ÿ Cáº§u Giáº¥y"
ğŸ¤– AI: Dá»±a trÃªn yÃªu cáº§u cá»§a báº¡n, tÃ´i gá»£i Ã½:
     1. QuÃ¡n Ä‚n Ngon - Cáº§u Giáº¥y
        ğŸ“ 123 ÄÆ°á»ng XYZ
        ğŸ’° GiÃ¡: 50k-100k/ngÆ°á»i
        â­ PhÃ¹ há»£p: 95%
```

**Cháº¿ Ä‘á»™ hoáº¡t Ä‘á»™ng:**
- ğŸŸ¢ **Ollama ON:** Full AI chat + semantic search
- ğŸŸ¡ **Ollama OFF:** Chá»‰ semantic search (váº«n chÃ­nh xÃ¡c)

### 3. Filter tÃ¬m kiáº¿m

- **Loáº¡i hÃ¬nh:** NhÃ  hÃ ng, Bar, Karaoke
- **Quáº­n:** TÃ¢y Há»“, HoÃ n Kiáº¿m, Cáº§u Giáº¥y, Ba ÄÃ¬nh...
- **Má»©c giÃ¡:** BÃ¬nh dÃ¢n, Trung bÃ¬nh, Cao cáº¥p

---

## âš™ï¸ Kiáº¿n trÃºc (ÄÆ¡n giáº£n)

File `.env`:

```env
# Ollama Configuration (Local LLM)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2:1.5b
```

---

## ğŸ› ï¸ Tech Stack

### Backend
- **FastAPI** - REST API framework
- **ChromaDB** - Vector database
- **Sentence Transformers** - Text embeddings
- **Ollama** - Local LLM inference

### Frontend
- **Streamlit** - Interactive web UI
- **Pandas** - Data processing

### LLM & RAG
- **Ollama** - Local LLM (qwen2, llama2, vinallama)
- **Retrieval-Augmented Generation** - RAG pattern

---

## ğŸ“Š Dá»¯ liá»‡u

- **1891 Ä‘á»‹a Ä‘iá»ƒm** tá»« TopGo.vn
- **Loáº¡i hÃ¬nh:** NhÃ  hÃ ng, Bar, Karaoke
- **Khu vá»±c:** HÃ  Ná»™i (cÃ¡c quáº­n ná»™i thÃ nh)
- **ThÃ´ng tin:** TÃªn, Ä‘á»‹a chá»‰, SÄT, giÃ¡, mÃ´ táº£, Ä‘Ã¡nh giÃ¡

---

## ğŸ”§ Development

### Crawl dá»¯ liá»‡u má»›i

```bash
python src/crawlers/topgo_crawler.py
```

### Táº¡o láº¡i embeddings

```bash
python scripts/rebuild_embeddings.py
```

### Xem thá»‘ng kÃª project

```bash
python scripts/project_stats.py
```

### Cháº¡y API Backend (Optional)

```bash
python scripts/run_api.py
# Hoáº·c: uvicorn src.api.main:app --reload
```

---

## ğŸ› Troubleshooting

### 1. Lá»—i "Ollama connection refused"

```bash
# Kiá»ƒm tra Ollama cÃ³ cháº¡y khÃ´ng
curl http://localhost:11434/api/tags

# Náº¿u khÃ´ng cháº¡y, khá»Ÿi Ä‘á»™ng láº¡i
ollama serve  # Linux/Mac
# Windows: Restart Ollama app
```

### 2. Model chÆ°a cÃ³

```bash
ollama pull qwen2:1.5b
```

### 3. Port 8000 Ä‘Ã£ dÃ¹ng

```bash
# Windows
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:8000 | xargs kill -9
```

---

## ï¿½ TÃ i liá»‡u bá»• sung

Xem thÃªm trong thÆ° má»¥c [docs/](docs/):

- ğŸ“˜ [QUICKSTART_SIMPLE.md](docs/QUICKSTART_SIMPLE.md) - Quickstart 3 bÆ°á»›c
- ğŸ”§ [SETUP_OLLAMA.md](docs/SETUP_OLLAMA.md) - Setup Ollama chi tiáº¿t
- ğŸ¨ [STREAMLIT_LOCAL.md](docs/STREAMLIT_LOCAL.md) - HÆ°á»›ng dáº«n Streamlit
- âš¡ [OPTIMIZATION_DONE.md](docs/OPTIMIZATION_DONE.md) - Tá»‘i Æ°u Ä‘Ã£ lÃ m
- ğŸ”¨ [RAG_IMPLEMENTATION.md](docs/RAG_IMPLEMENTATION.md) - RAG pipeline
- ğŸ“Š [API_BACKEND_COMPLETE.md](docs/API_BACKEND_COMPLETE.md) - API docs

---

## ï¿½ğŸ“ To-Do

- [ ] Add more filters (cuisine type, rating)
- [ ] Multi-language support
- [ ] User feedback system
- [ ] Recommendation history
- [ ] Mobile responsive UI

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¥ Contributors

- Your Name - Initial work

---

## ğŸ™ Acknowledgments

- TopGo.vn - Data source
- Ollama - Local LLM framework
- ChromaDB - Vector database
- Sentence Transformers - Embeddings

---

## ğŸ“ Support

- ğŸ“– Docs: [SETUP_OLLAMA.md](SETUP_OLLAMA.md)
- ğŸ› Issues: Create an issue on GitHub
- ğŸ’¬ Discord: [Ollama Community](https://discord.gg/ollama)
