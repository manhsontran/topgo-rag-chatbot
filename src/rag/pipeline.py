"""
RAG Pipeline: Retrieval-Augmented Generation for restaurant recommendations
"""
from typing import Dict, List, Optional
from pathlib import Path
import sys

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.embeddings.search_engine import RestaurantSearchEngine
from src.llm.ollama_client import OllamaClient
from src.rag.prompts import PromptTemplates


class RAGPipeline:
    """
    RAG Pipeline combining semantic search with LLM generation
    """
    
    def __init__(
        self,
        model: str = "qwen2:1.5b",
        ollama_url: str = "http://localhost:11434",
        search_top_k: int = 5
    ):
        """
        Initialize RAG pipeline with Ollama (Local LLM)
        
        Args:
            model: Ollama model name (qwen2:1.5b, llama2, mistral, etc.)
            ollama_url: Ollama API URL (default: http://localhost:11434)
            search_top_k: Number of restaurants to retrieve
        """
        print("üîÑ Initializing RAG Pipeline (Ollama Local LLM)...")
        
        # Initialize components
        self.search_engine = RestaurantSearchEngine()
        self.prompt_templates = PromptTemplates()
        self.search_top_k = search_top_k
        self.model = model
        
        # Initialize Ollama
        self.llm = OllamaClient(base_url=ollama_url, model=model)
        
        # Check Ollama connection
        if not self.llm.check_connection():
            print("‚ö†Ô∏è  Warning: Ollama is not running!")
            print("   Start Ollama with: ollama serve")
            print("   Pipeline will work in search-only mode")
            self.ollama_available = False
        else:
            print(f"‚úÖ Ollama connected! Model: {model}")
            self.ollama_available = True
        
        print("‚úÖ RAG Pipeline ready!\n")
    
    @property
    def llm_available(self) -> bool:
        """Check if Ollama LLM is available"""
        return self.ollama_available
    
    def _llm_generate(self, prompt: str, system_prompt: str = "", temperature: float = 0.7, max_tokens: int = 800) -> str:
        """
        Generate text using Ollama
        
        Args:
            prompt: User prompt
            system_prompt: System prompt
            temperature: Temperature
            max_tokens: Max tokens
            
        Returns:
            Generated text
        """
        return self.llm.generate(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens
        )
    
    def retrieve(
        self,
        query: str,
        filters: Optional[Dict] = None,
        top_k: Optional[int] = None
    ) -> List[Dict]:
        """
        Retrieve relevant restaurants using semantic search
        
        Args:
            query: User query
            filters: Optional filters (business_type, district, price_range)
            top_k: Number of results (defaults to search_top_k)
            
        Returns:
            List of relevant restaurants
        """
        k = top_k or self.search_top_k
        
        # Use search with filters - ChromaDB will handle multiple filters
        return self.search_engine.search(query, n_results=k, filters=filters)
    
    def generate(
        self,
        query: str,
        context_restaurants: List[Dict],
        temperature: float = 0.7,
        max_tokens: int = 800
    ) -> str:
        """
        Generate answer using LLM with retrieved context
        
        Args:
            query: User query
            context_restaurants: Retrieved restaurants
            temperature: LLM temperature
            max_tokens: Max tokens to generate
            
        Returns:
            Generated answer
        """
        
        if not self.llm_available:
            # Return search results without LLM
            return self._format_search_only_response(context_restaurants)
        
        # IMPORTANT: If no results, return directly without calling LLM to avoid hallucination
        if not context_restaurants:
            return """R·∫•t ti·∫øc, t√¥i kh√¥ng t√¨m th·∫•y ƒë·ªãa ƒëi·ªÉm n√†o ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n trong c∆° s·ªü d·ªØ li·ªáu hi·ªán t·∫°i.

B·∫°n c√≥ th·ªÉ th·ª≠:
- M·ªü r·ªông khu v·ª±c t√¨m ki·∫øm (th·ª≠ c√°c qu·∫≠n kh√°c)
- ƒêi·ªÅu ch·ªânh m·ª©c gi√°  
- Thay ƒë·ªïi lo·∫°i h√¨nh (nh√† h√†ng, bar, karaoke)

Ho·∫∑c cho t√¥i bi·∫øt th√™m chi ti·∫øt v·ªÅ nhu c·∫ßu c·ªßa b·∫°n ƒë·ªÉ t√¥i c√≥ th·ªÉ t∆∞ v·∫•n t·ªët h∆°n."""
        
        # Build prompt with restaurant data
        prompt = PromptTemplates.build_prompt(query, context_restaurants)
        
        # Generate with LLM
        response = self._llm_generate(
            prompt=prompt,
            system_prompt=PromptTemplates.SYSTEM_PROMPT,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # Post-process: Validate that LLM didn't change district names
        if context_restaurants:
            # Get actual districts from data
            actual_districts = set(r.get('district', '') for r in context_restaurants)
            
            # Common LLM mistakes mapping
            district_corrections = {
                'ho√†ng kim': 'Ho√†n Ki·∫øm',
                'hoang kim': 'Ho√†n Ki·∫øm', 
                'c·∫ßu g·ªó': 'C·∫ßu Gi·∫•y',
                'cau go': 'C·∫ßu Gi·∫•y',
                'c·∫ßu gi·∫ßy': 'C·∫ßu Gi·∫•y',
                't√¢y h√¥': 'T√¢y H·ªì',
                'tay ho': 'T√¢y H·ªì',
                'ƒë·ªìng ƒëa': 'ƒê·ªëng ƒêa',
                'dong da': 'ƒê·ªëng ƒêa'
            }
            
            # Check and fix common mistakes in response
            response_lower = response.lower()
            for wrong, correct in district_corrections.items():
                if wrong in response_lower and correct in actual_districts:
                    # Case-insensitive replacement
                    import re
                    pattern = re.compile(re.escape(wrong), re.IGNORECASE)
                    response = pattern.sub(correct, response)
                    print(f"‚ö†Ô∏è  Fixed LLM mistake: '{wrong}' ‚Üí '{correct}'")
        
        # Post-process: Remove "kh√¥ng t√¨m th·∫•y" if restaurants were introduced
        if context_restaurants and len(response) > 100:
            # If response is long and has restaurant info, remove any "not found" messages
            negative_phrases = [
                "R·∫•t ti·∫øc, t√¥i kh√¥ng t√¨m th·∫•y",
                "Xin l·ªói, kh√¥ng t√¨m th·∫•y",
                "Kh√¥ng t√¨m th·∫•y ƒë·ªãa ƒëi·ªÉm ph√π h·ª£p"
            ]
            for phrase in negative_phrases:
                if phrase in response:
                    # Split by the negative phrase and keep only the part before it
                    response = response.split(phrase)[0].strip()
        
        return response
    
    def _classify_query_with_llm(self, query: str) -> Dict:
        """
        Use LLM to classify user query and determine if restaurant search is needed
        
        Args:
            query: User query
            
        Returns:
            Dict with 'needs_search' (bool) and 'response_type' (str)
        """
        if not self.llm_available:
            # Fallback: assume all queries need search
            return {'needs_search': True, 'response_type': 'restaurant_query'}
        
        # Pre-classification using keywords (faster and more reliable)
        query_lower = query.lower()
        
        # Strong restaurant/bar/karaoke indicators
        restaurant_keywords = [
            'nh√† h√†ng', 'qu√°n ƒÉn', 'qu√°n', 'bar', 'pub', 'karaoke',
            'buffet', 'restaurant', 'cafe', 'qu√°n cafe',
            'ƒÉn', 'm√≥n', 'ƒë·ªì ƒÉn', 'th·ª©c ƒÉn', 'b·ªØa', 'c∆°m', 'ph·ªü',
            'b√∫n', 'm√¨', 'l·∫©u', 'n∆∞·ªõng', 'dimsum',
            'qu·∫≠n', '·ªü ƒë√¢u', 'g·∫ßn', 'ph√π h·ª£p', 't·ªët', 'ngon',
            'gi√° r·∫ª', 'b√¨nh d√¢n', 'sang tr·ªçng', 'cao c·∫•p',
            'trung b√¨nh', 'g·ª£i √Ω', 'gi·ªõi thi·ªáu', 't√¨m', 'cho t√¥i',
            't√¢y', '√Ω', 'nh·∫≠t', 'h√†n', 'trung', 'vi·ªát', '√¢u', '√°',
            'c·∫ßu gi·∫•y', 't√¢y h·ªì', 'ho√†n ki·∫øm', 'ba ƒë√¨nh', 'ƒë·ªëng ƒëa',
            'hai b√† tr∆∞ng', 'thanh xu√¢n', 'long bi√™n', 'ho√†ng mai'
        ]
        
        # Check if query contains restaurant-related keywords
        has_restaurant_keyword = any(keyword in query_lower for keyword in restaurant_keywords)
        
        # Strong greeting indicators (only if NO restaurant keywords)
        greeting_keywords = ['xin ch√†o', 'ch√†o', 'hello', 'hi', 'hey']
        is_pure_greeting = any(query_lower.strip() == greeting for greeting in greeting_keywords)
        
        # If has restaurant keywords, classify as restaurant_query immediately
        if has_restaurant_keyword:
            return {'needs_search': True, 'response_type': 'restaurant_query'}
        
        # If pure greeting only
        if is_pure_greeting:
            return {'needs_search': False, 'response_type': 'greeting'}
        
        # Otherwise, use LLM for ambiguous cases
        classification_prompt = f"""Ph√¢n t√≠ch c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√† tr·∫£ l·ªùi theo format JSON:

C√¢u h·ªèi: "{query}"

H√£y x√°c ƒë·ªãnh:
1. Ng∆∞·ªùi d√πng c√≥ ƒëang T√åM KI·∫æM nh√† h√†ng/qu√°n bar/karaoke c·ª• th·ªÉ kh√¥ng?
2. Hay ch·ªâ ƒëang ch√†o h·ªèi/h·ªèi th√¥ng tin chung v·ªÅ chatbot?

Tr·∫£ l·ªùi CH√çNH X√ÅC theo format JSON n√†y (kh√¥ng gi·∫£i th√≠ch th√™m):
{{
    "needs_search": true/false,
    "response_type": "greeting" ho·∫∑c "general_question" ho·∫∑c "restaurant_query",
    "reasoning": "l√Ω do ng·∫Øn g·ªçn"
}}

V√ç D·ª§:
- "hello" ‚Üí {{"needs_search": false, "response_type": "greeting", "reasoning": "Ch·ªâ ch√†o h·ªèi"}}
- "b·∫°n l√† ai" ‚Üí {{"needs_search": false, "response_type": "general_question", "reasoning": "H·ªèi v·ªÅ chatbot"}}
- "t√¨m nh√† h√†ng b√¨nh d√¢n" ‚Üí {{"needs_search": true, "response_type": "restaurant_query", "reasoning": "T√¨m nh√† h√†ng c·ª• th·ªÉ"}}
- "qu√°n n√†o ngon ·ªü c·∫ßu gi·∫•y" ‚Üí {{"needs_search": true, "response_type": "restaurant_query", "reasoning": "H·ªèi v·ªÅ ƒë·ªãa ƒëi·ªÉm"}}

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng c√≥ text kh√°c."""

        try:
            response = self._llm_generate(
                prompt=classification_prompt,
                system_prompt="B·∫°n l√† AI ph√¢n lo·∫°i c√¢u h·ªèi. Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch th√™m.",
                temperature=0.1,
                max_tokens=150
            )
            
            # Parse JSON response
            import json
            import re
            
            # Extract JSON from response
            json_match = re.search(r'\{[^}]+\}', response)
            if json_match:
                result = json.loads(json_match.group())
                return result
            else:
                # Fallback if can't parse
                return {'needs_search': True, 'response_type': 'restaurant_query'}
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Classification error: {e}")
            # Fallback: assume restaurant query
            return {'needs_search': True, 'response_type': 'restaurant_query'}
    
    def _generate_conversational_response(self, query: str, response_type: str) -> str:
        """
        Generate natural conversational response using LLM (no restaurant search)
        
        Args:
            query: User query
            response_type: Type of response (greeting, general_question)
            
        Returns:
            Natural response from LLM
        """
        if not self.llm_available:
            return "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω t∆∞ v·∫•n nh√† h√†ng t·∫°i H√† N·ªôi. B·∫°n c·∫ßn t√¨m lo·∫°i ƒë·ªãa ƒëi·ªÉm n√†o?"
        
        # Check if query is about non-restaurant topics
        non_restaurant_keywords = [
            'to√°n', 't√≠nh', '+', '-', '*', '/', 'b·∫±ng m·∫•y', 'k·∫øt qu·∫£',
            'th·ªùi ti·∫øt', 'tr·ªùi', 'n·∫Øng', 'm∆∞a', 'nhi·ªát ƒë·ªô',
            'tin t·ª©c', 'b√≥ng ƒë√°', 'ch√≠nh tr·ªã', 'kinh t·∫ø',
            'l·ªãch s·ª≠', 'ƒë·ªãa l√Ω', 'khoa h·ªçc', 'v·∫≠t l√Ω', 'h√≥a h·ªçc',
            'code', 'l·∫≠p tr√¨nh', 'python', 'javascript'
        ]
        
        query_lower = query.lower()
        is_non_restaurant = any(keyword in query_lower for keyword in non_restaurant_keywords)
        
        if is_non_restaurant:
            return """Xin l·ªói b·∫°n, t√¥i l√† chuy√™n vi√™n t∆∞ v·∫•n v·ªÅ nh√† h√†ng, qu√°n bar v√† karaoke t·∫°i H√† N·ªôi. T√¥i kh√¥ng c√≥ kh·∫£ nƒÉng tr·∫£ l·ªùi v·ªÅ c√°c v·∫•n ƒë·ªÅ kh√°c.

T√¥i ch·ªâ c√≥ th·ªÉ gi√∫p b·∫°n:
- T√¨m nh√† h√†ng ph√π h·ª£p
- G·ª£i √Ω qu√°n bar, karaoke
- T∆∞ v·∫•n ƒë·ªãa ƒëi·ªÉm ƒÉn u·ªëng theo nhu c·∫ßu

B·∫°n c·∫ßn t√¨m lo·∫°i ƒë·ªãa ƒëi·ªÉm n√†o?"""
        
        # For greetings and general questions about the bot
        if response_type == 'greeting':
            return """Xin ch√†o! T√¥i l√† tr·ª£ l√Ω AI chuy√™n t∆∞ v·∫•n v·ªÅ nh√† h√†ng, qu√°n bar v√† karaoke t·∫°i H√† N·ªôi.

T√¥i c√≥ th·ªÉ gi√∫p b·∫°n:
- T√¨m nh√† h√†ng theo lo·∫°i h√¨nh, qu·∫≠n, m·ª©c gi√°
- G·ª£i √Ω ƒë·ªãa ƒëi·ªÉm ph√π h·ª£p cho c√°c d·ªãp ƒë·∫∑c bi·ªát
- T∆∞ v·∫•n qu√°n bar, karaoke

B·∫°n ƒëang t√¨m lo·∫°i ƒë·ªãa ƒëi·ªÉm n√†o?"""
        
        conversation_prompt = f"""Ng∆∞·ªùi d√πng h·ªèi: {query}

Tr·∫£ l·ªùi NG·∫ÆN G·ªåN (2-3 c√¢u) b·∫±ng TI·∫æNG VI·ªÜT:

N·∫øu h·ªèi v·ªÅ B·∫†N:
- Gi·ªõi thi·ªáu: "T√¥i l√† tr·ª£ l√Ω AI chuy√™n t∆∞ v·∫•n nh√† h√†ng, qu√°n bar v√† karaoke t·∫°i H√† N·ªôi."
- Ch·ª©c nƒÉng: "T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m ƒë·ªãa ƒëi·ªÉm ƒÉn u·ªëng ph√π h·ª£p v·ªõi nhu c·∫ßu."
- H·ªèi ng∆∞·ª£c: "B·∫°n c·∫ßn t√¨m lo·∫°i ƒë·ªãa ƒëi·ªÉm n√†o?"

KH√îNG li·ªát k√™ nh√† h√†ng c·ª• th·ªÉ."""

        response = self._llm_generate(
            prompt=conversation_prompt,
            system_prompt="B·∫°n l√† tr·ª£ l√Ω AI chuy√™n t∆∞ v·∫•n nh√† h√†ng t·∫°i H√† N·ªôi. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, th√¢n thi·ªán.",
            temperature=0.5,
            max_tokens=150
        )
        
        return response
    
    def _validate_district_in_query(self, query: str) -> tuple:
        """
        Check if query mentions a valid Hanoi district
        Returns (is_valid, invalid_district_name)
        """
        VALID_DISTRICTS = {
            'ba ƒë√¨nh', 'ba dinh', 'ho√†n ki·∫øm', 'hoan kiem', 't√¢y h·ªì', 'tay ho',
            'long bi√™n', 'long bien', 'c·∫ßu gi·∫•y', 'cau giay', 'ƒë·ªëng ƒëa', 'dong da',
            'hai b√† tr∆∞ng', 'hai ba trung', 'ho√†ng mai', 'hoang mai', 'thanh xu√¢n', 'thanh xuan',
            't·ª´ li√™m', 'tu liem', 'nam t·ª´ li√™m', 'nam tu liem', 'b·∫Øc t·ª´ li√™m', 'bac tu liem',
            's√≥c s∆°n', 'soc son', 'ƒë√¥ng anh', 'dong anh', 'gia l√¢m', 'gia lam',
            'thanh tr√¨', 'thanh tri', 'h√† ƒë√¥ng', 'ha dong', 's∆°n t√¢y', 'son tay',
            'ba v√¨', 'ba vi', 'ph√∫c th·ªç', 'phuc tho', 'ƒëan ph∆∞·ª£ng', 'dan phuong',
            'ho√†i ƒë·ª©c', 'hoai duc', 'qu·ªëc oai', 'quoc oai', 'th·∫°ch th·∫•t', 'thach that',
            'ch∆∞∆°ng m·ªπ', 'chuong my', 'thanh oai', 'm·ªπ ƒë·ª©c', 'my duc',
            '·ª©ng h√≤a', 'ung hoa', 'th∆∞·ªùng t√≠n', 'thuong tin', 'ph√∫ xuy√™n', 'phu xuyen',
            'm√™ linh', 'me linh'
        }
        
        query_lower = query.lower()
        
        # Check if query mentions district/qu·∫≠n
        if 'qu·∫≠n' in query_lower or 'district' in query_lower:
            # Extract potential district name after these keywords
            import re
            # Match "qu·∫≠n X" or "district X" - capture everything after qu·∫≠n/district until space or punctuation
            # This will catch numbers, special chars, and text
            pattern = r'(?:qu·∫≠n|district)\s+([^,\.\?!\n]+?)(?:\s+(?:nh√†|qu√°n|bar|restaurant|karaoke|gi√°|r·∫ª|sang|$)|$)'
            
            match = re.search(pattern, query_lower)
            if match:
                mentioned_district = match.group(1).strip()
                
                # Additional cleanup: remove trailing words that might be captured
                # Split and take only first 3 words max for district name
                district_words = mentioned_district.split()
                if len(district_words) > 3:
                    mentioned_district = ' '.join(district_words[:3])
                
                mentioned_district = mentioned_district.strip()
                
                # Check if it's a valid district
                if mentioned_district and mentioned_district not in VALID_DISTRICTS:
                    print(f"‚ö†Ô∏è  Invalid district in query: '{mentioned_district}'")
                    return False, mentioned_district
        
        return True, ""
    
    def _extract_filters_from_query(self, query: str) -> Dict:
        """
        Use LLM to extract filters from user query
        
        Args:
            query: User query
            
        Returns:
            Dict with extracted filters (district, business_type, price_range)
        """
        if not self.llm_available:
            return {}
        
        # Pre-validate district in query
        is_valid, invalid_district = self._validate_district_in_query(query)
        if not is_valid:
            print(f"   ‚õî Query contains invalid district '{invalid_district}' - returning special marker")
            return {'_invalid_district': invalid_district}
        
        extraction_prompt = f"""Ph√¢n t√≠ch c√¢u h·ªèi v√† tr√≠ch xu·∫•t th√¥ng tin:

C√¢u h·ªèi: "{query}"

T√¨m c√°c th√¥ng tin sau (N·∫æU C√ì trong c√¢u h·ªèi):
1. Qu·∫≠n (CH·ªà c√°c qu·∫≠n H·ª¢P L·ªÜ ·ªü H√† N·ªôi): T√¢y H·ªì, Ho√†n Ki·∫øm, C·∫ßu Gi·∫•y, Ba ƒê√¨nh, ƒê·ªëng ƒêa, Hai B√† Tr∆∞ng, Thanh Xu√¢n, Long Bi√™n, Ho√†ng Mai
2. Lo·∫°i: restaurant (nh√† h√†ng), bar (qu√°n bar), karaoke  
3. Gi√°: binh_dan (b√¨nh d√¢n/r·∫ª), trung_binh (trung b√¨nh), cao_cap (sang/cao c·∫•p)

QUAN TR·ªåNG:
- CH·ªà tr√≠ch xu·∫•t qu·∫≠n N·∫æU n√≥ l√† qu·∫≠n TH·∫¨T c·ªßa H√† N·ªôi
- N·∫æU qu·∫≠n KH√îNG H·ª¢P L·ªÜ (v√≠ d·ª•: "sao H·ªèa", "sao Mars", v.v.) ‚Üí KH√îNG tr·∫£ v·ªÅ district
- KH√îNG t·ª± s·ª≠a ho·∫∑c ƒëo√°n t√™n qu·∫≠n
- price_range: "binh_dan" ho·∫∑c "trung_binh" ho·∫∑c "cao_cap" (ch·ªØ th∆∞·ªùng, g·∫°ch d∆∞·ªõi)
- business_type: "restaurant" ho·∫∑c "bar" ho·∫∑c "karaoke"
- N·∫øu KH√îNG ch·∫Øc ch·∫Øn ‚Üí b·ªè qua key ƒë√≥

Tr·∫£ l·ªùi CH√çNH X√ÅC theo format JSON (KH√îNG th√™m text):
{{
    "district": "T√¢y H·ªì",
    "business_type": "restaurant",
    "price_range": "binh_dan"
}}

B√¢y gi·ªù tr√≠ch xu·∫•t (ch·ªâ JSON):"""

        try:
            response = self._llm_generate(
                prompt=extraction_prompt,
                system_prompt="Tr·∫£ v·ªÅ JSON. KH√îNG gi·∫£i th√≠ch.",
                temperature=0.1,
                max_tokens=100
            )
            
            # Parse JSON
            import json
            import re
            
            json_match = re.search(r'\{[^}]*\}', response, re.DOTALL)
            if json_match:
                filters = json.loads(json_match.group())
                
                # Valid Hanoi districts (normalized)
                VALID_DISTRICTS = {
                    'ba ƒë√¨nh', 'ba dinh', 'ho√†n ki·∫øm', 'hoan kiem', 't√¢y h·ªì', 'tay ho',
                    'long bi√™n', 'long bien', 'c·∫ßu gi·∫•y', 'cau giay', 'ƒë·ªëng ƒëa', 'dong da',
                    'hai b√† tr∆∞ng', 'hai ba trung', 'ho√†ng mai', 'hoang mai', 'thanh xu√¢n', 'thanh xuan',
                    't·ª´ li√™m', 'tu liem', 'nam t·ª´ li√™m', 'nam tu liem', 'b·∫Øc t·ª´ li√™m', 'bac tu liem',
                    's√≥c s∆°n', 'soc son', 'ƒë√¥ng anh', 'dong anh', 'gia l√¢m', 'gia lam',
                    'thanh tr√¨', 'thanh tri', 'h√† ƒë√¥ng', 'ha dong', 's∆°n t√¢y', 'son tay',
                    'ba v√¨', 'ba vi', 'ph√∫c th·ªç', 'phuc tho', 'ƒëan ph∆∞·ª£ng', 'dan phuong',
                    'ho√†i ƒë·ª©c', 'hoai duc', 'qu·ªëc oai', 'quoc oai', 'th·∫°ch th·∫•t', 'thach that',
                    'ch∆∞∆°ng m·ªπ', 'chuong my', 'thanh oai', 'm·ªπ ƒë·ª©c', 'my duc',
                    '·ª©ng h√≤a', 'ung hoa', 'th∆∞·ªùng t√≠n', 'thuong tin', 'ph√∫ xuy√™n', 'phu xuyen',
                    'm√™ linh', 'me linh', 'th∆∞·ªùng t√≠n', 'thuong tin'
                }
                
                # Validate and clean filters
                valid_filters = {}
                
                # Check district - MUST be valid Hanoi district
                if 'district' in filters and filters['district']:
                    dist = filters['district'].strip()
                    dist_normalized = dist.lower()
                    
                    # Only accept if it's a real Hanoi district
                    if dist_normalized in VALID_DISTRICTS:
                        valid_filters['district'] = dist
                    else:
                        print(f"‚ö†Ô∏è  Invalid district from LLM: '{dist}' - not in Hanoi district list")
                
                # Check business_type
                if 'business_type' in filters and filters['business_type']:
                    btype = filters['business_type'].strip().lower()
                    if btype in ['restaurant', 'bar', 'karaoke']:
                        valid_filters['business_type'] = btype
                
                # Check price_range
                if 'price_range' in filters and filters['price_range']:
                    price = filters['price_range'].strip()
                    # Map to database format (lowercase with underscore)
                    price_map = {
                        'binh dan': 'binh_dan',
                        'binh_dan': 'binh_dan',
                        'trung binh': 'trung_binh',
                        'trung_binh': 'trung_binh',
                        'cao cap': 'cao_cap',
                        'cao_cap': 'cao_cap'
                    }
                    normalized = price_map.get(price.lower(), None)
                    if normalized:
                        valid_filters['price_range'] = normalized
                
                return valid_filters
            else:
                return {}
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Filter extraction error: {e}")
            return {}
    
    def answer(
        self,
        query: str,
        filters: Optional[Dict] = None,
        top_k: Optional[int] = None,
        temperature: float = 0.7,
        max_tokens: int = 800,
        return_sources: bool = True
    ) -> Dict:
        """
        Complete RAG pipeline: retrieve + generate
        
        Args:
            query: User query
            filters: Optional filters (if None, will auto-extract from query)
            top_k: Number of restaurants to retrieve
            temperature: LLM temperature
            max_tokens: Max tokens
            return_sources: Whether to return source restaurants
            
        Returns:
            Dictionary with answer and optionally sources
        """
        # Step 1: Classify query using LLM
        print(f"ü§î Analyzing query: '{query}'")
        classification = self._classify_query_with_llm(query)
        print(f"   Classification: {classification.get('response_type')} (needs_search: {classification.get('needs_search')})")
        
        # Step 2: Handle based on classification
        if not classification.get('needs_search', True):
            # No search needed - generate conversational response
            print(f"üí¨ Generating conversational response...")
            response_type = classification.get('response_type', 'greeting')
            answer = self._generate_conversational_response(query, response_type)
            return {
                'query': query,
                'answer': answer,
                'num_sources': 0,
                'sources': [] if return_sources else None
            }
        
        # Step 3: Auto-extract filters from query if not provided
        if not filters:
            print(f"üîç Extracting filters from query...")
            filters = self._extract_filters_from_query(query)
            
            # Check if invalid district was detected
            if filters and '_invalid_district' in filters:
                invalid_district = filters['_invalid_district']
                print(f"   ‚ùå Invalid district detected: '{invalid_district}'")
                answer = f"Xin l·ªói, qu·∫≠n '{invalid_district}' kh√¥ng t·ªìn t·∫°i t·∫°i H√† N·ªôi. H√† N·ªôi c√≥ c√°c qu·∫≠n sau: Ho√†n Ki·∫øm, Ba ƒê√¨nh, T√¢y H·ªì, C·∫ßu Gi·∫•y, ƒê·ªëng ƒêa, Hai B√† Tr∆∞ng, Thanh Xu√¢n, Long Bi√™n, Ho√†ng Mai, H√† ƒê√¥ng v√† c√°c huy·ªán ngo·∫°i th√†nh. B·∫°n c√≥ th·ªÉ ch·ªçn m·ªôt qu·∫≠n kh√°c."
                return {
                    'query': query,
                    'answer': answer,
                    'num_sources': 0,
                    'sources': [] if return_sources else None
                }
            
            if filters:
                print(f"   Extracted filters: {filters}")
        
        # Do search + generation
        print(f"üîç Searching for: '{query}'")
        restaurants = self.retrieve(query, filters, top_k)
        print(f"   Found {len(restaurants)} relevant restaurants")
        
        # Generate response with error handling
        if self.llm_available:
            try:
                print(f"ü§ñ Generating response with LLM...")
                answer = self.generate(query, restaurants, temperature, max_tokens)
            except Exception as e:
                print(f"‚ö†Ô∏è  LLM error (quota exceeded or other): {str(e)[:100]}")
                print(f"üìã Fallback: Returning search results without LLM...")
                answer = self._format_search_only_response(restaurants)
        else:
            print(f"üìã No LLM available - Returning search results...")
            answer = self._format_search_only_response(restaurants)
        
        # Build result
        result = {
            'query': query,
            'answer': answer,
            'num_sources': len(restaurants)
        }
        
        if return_sources:
            result['sources'] = restaurants
        
        return result
    
    def _format_search_only_response(self, restaurants: List[Dict]) -> str:
        """Format response when Ollama is not available"""
        if not restaurants:
            return "Kh√¥ng t√¨m th·∫•y ƒë·ªãa ƒëi·ªÉm ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n."
        
        response = f"T√¨m th·∫•y {len(restaurants)} ƒë·ªãa ƒëi·ªÉm ph√π h·ª£p:\n\n"
        
        for i, resto in enumerate(restaurants, 1):
            response += f"{i}. {resto['name']}\n"
            response += f"   - Lo·∫°i: {resto['business_type'].title()}\n"
            response += f"   - Qu·∫≠n: {resto['district']}\n"
            response += f"   - Gi√°: {resto['price_range'].replace('_', ' ').title()}\n"
            response += f"   - SƒêT: {resto['phone']}\n"
            response += f"   - ƒê·ªãa ch·ªâ: {resto['address']}\n"
            
            if resto.get('cuisine_type'):
                cuisines = ', '.join([c for c in resto['cuisine_type'] if c])
                if cuisines:
                    response += f"   - ·∫®m th·ª±c: {cuisines}\n"
            
            response += "\n"
        
        return response
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        filters: Optional[Dict] = None,
        top_k: Optional[int] = None
    ) -> Dict:
        """
        Chat with conversation history
        
        Args:
            messages: Conversation history [{'role': 'user', 'content': '...'}]
            filters: Optional filters
            top_k: Number of results
            
        Returns:
            Response dict
        """
        # Get last user message
        user_messages = [m for m in messages if m.get('role') == 'user']
        if not user_messages:
            return {'error': 'No user message found'}
        
        query = user_messages[-1].get('content', '')
        
        # Use regular answer method
        return self.answer(query, filters, top_k)


def main():
    """Test RAG pipeline"""
    print("=" * 80)
    print("ü§ñ RAG PIPELINE TEST")
    print("=" * 80)
    
    # Initialize pipeline
    pipeline = RAGPipeline(model="llama2", search_top_k=5)
    
    # Test queries
    test_queries = [
        {
            'query': 'T√¨m nh√† h√†ng Vi·ªát Nam b√¨nh d√¢n cho gia ƒë√¨nh ·ªü C·∫ßu Gi·∫•y',
            'filters': None
        },
        {
            'query': 'Qu√°n karaoke sang tr·ªçng c√≥ ph√≤ng VIP',
            'filters': {'business_type': 'karaoke'}
        },
        {
            'query': 'Bar c√≥ view ƒë·∫πp ph√π h·ª£p h·∫πn h√≤ ·ªü Ho√†n Ki·∫øm',
            'filters': {'district': 'Ho√†n Ki·∫øm'}
        },
        {
            'query': 'N∆°i t·ªï ch·ª©c ti·ªác c√¥ng ty gi√° b√¨nh d√¢n',
            'filters': {'price_range': 'Binh Dan'}
        }
    ]
    
    for i, test in enumerate(test_queries, 1):
        print("\n" + "=" * 80)
        print(f"TEST {i}")
        print("=" * 80)
        print(f"Query: {test['query']}")
        if test.get('filters'):
            print(f"Filters: {test['filters']}")
        print("-" * 80)
        
        # Get answer
        result = pipeline.answer(
            query=test['query'],
            filters=test.get('filters'),
            temperature=0.7,
            return_sources=True
        )
        
        # Display answer
        print("\nüìù ANSWER:")
        print(result['answer'])
        
        # Display sources
        print(f"\nüìö SOURCES ({result['num_sources']} restaurants):")
        for j, source in enumerate(result.get('sources', [])[:3], 1):
            print(f"\n{j}. {source['name']}")
            print(f"   {source['business_type'].title()} | {source['district']} | {source['price_range'].replace('_', ' ').title()}")
            print(f"   üìû {source['phone']}")
            print(f"   üéØ Similarity: {source['similarity_score']:.2%}")
        
        print("\n" + "=" * 80)
        
        # Pause between tests
        if i < len(test_queries):
            input("\n‚èé Press Enter for next test...")
    
    print("\n‚úÖ All tests complete!")


if __name__ == "__main__":
    main()
