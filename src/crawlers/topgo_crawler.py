# src/crawlers/topgo_crawler.py
import requests
from bs4 import BeautifulSoup
import json
import time
from tqdm import tqdm
from typing import List, Dict
import re

class TopGoCrawler:
    def __init__(self):
        self.base_url = "https://topgo.vn"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_restaurant_urls(self, max_pages_per_category: int | None = None) -> List[str]:
        """L·∫•y danh s√°ch URLs c·ªßa t·∫•t c·∫£ nh√† h√†ng
        
        Args:
            max_pages_per_category: S·ªë trang t·ªëi ƒëa crawl cho m·ªói category (None = kh√¥ng gi·ªõi h·∫°n)
        """
        urls = []
        seen_urls = set()  # Track URLs to detect duplicates
        
        # Crawl t·ª´ c√°c category pages
        categories = [
            '/category/nha-hang/',
            '/category/karaoke/',
            '/category/bar-lounge/',
        ]
        
        for category in categories:
            print(f"\nüìÇ Category: {category}")
            page = 1
            pages_crawled = 0
            no_new_urls_count = 0  # Count pages with no new URLs
            
            while True:
                # Check limit n·∫øu c√≥
                if max_pages_per_category and pages_crawled >= max_pages_per_category:
                    print(f"  ‚è∏Ô∏è  Reached limit of {max_pages_per_category} pages")
                    break
                
                # Stop if we've seen 10 consecutive pages with no new URLs (pagination loop detected)
                if no_new_urls_count >= 10:
                    print(f"  üîÑ Detected pagination loop - stopping category")
                    break
                
                url = f"{self.base_url}{category}page/{page}/"
                page_info = f"  Page {page}" + (f"/{max_pages_per_category}" if max_pages_per_category else "")
                print(f"{page_info}: {url}")
                
                try:
                    response = self.session.get(url, timeout=10)
                    if response.status_code != 200:
                        print(f"  ‚ö†Ô∏è Status {response.status_code}, stopping category")
                        break
                    
                    soup = BeautifulSoup(response.content.decode('utf-8'), 'html.parser')
                    
                    # T√¨m links ƒë·∫øn restaurant pages
                    posts = soup.find_all('div', class_='column-post')
                    if not posts:
                        print(f"  ‚ö†Ô∏è No posts found, stopping category")
                        break
                    
                    new_urls_this_page = 0
                    for post in posts:
                        link = post.find('a', href=True)
                        if link and link['href'].startswith('https://topgo.vn/'):
                            # Lo·∫°i b·ªè c√°c link kh√¥ng ph·∫£i nh√† h√†ng
                            url_lower = link['href'].lower()
                            if not any(skip in url_lower for skip in ['/category/', '/wp-', '/tag/', '/combo-deals/', '/top-goi-y/', '/nha-tai-tro/', '/blog/', '/diem-nhan/', '/trip/']):
                                if link['href'] not in seen_urls:
                                    seen_urls.add(link['href'])
                                    urls.append(link['href'])
                                    new_urls_this_page += 1
                    
                    if new_urls_this_page == 0:
                        no_new_urls_count += 1
                        print(f"  ‚ö†Ô∏è  No new URLs (duplicate page {no_new_urls_count}/10)")
                    else:
                        no_new_urls_count = 0  # Reset counter
                        print(f"  ‚úì Found {new_urls_this_page} NEW restaurant links")
                    
                    page += 1
                    pages_crawled += 1
                    time.sleep(0.5)  # Rate limiting
                    
                except Exception as e:
                    print(f"  ‚ùå Error: {e}")
                    break
        
        unique_urls = list(set(urls))  # Remove duplicates
        print(f"\n‚úÖ Total unique restaurant URLs: {len(unique_urls)}")
        return unique_urls
    
    def extract_phone(self, soup) -> str:
        """Extract s·ªë ƒëi·ªán tho·∫°i"""
        phone_link = soup.find('a', href=re.compile(r'tel:'))
        if phone_link:
            return phone_link.get('href', '').replace('tel:', '')
        return ""
    
    def extract_address(self, soup) -> str:
        """Extract ƒë·ªãa ch·ªâ"""
        # Pattern 1: T√¨m trong column-post-adress (tr√™n trang listing)
        addr_div = soup.find('div', class_='column-post-adress')
        if addr_div:
            text = addr_div.text.strip()
            # Clean: lo·∫°i b·ªè s·ªë ƒëi·ªán tho·∫°i duplicate
            text = re.sub(r'\d{10,}', '', text)
            text = re.sub(r'\s+', ' ', text).strip()
            if len(text) > 10:
                return text[:300]
        
        # Pattern 2: T√¨m text c√≥ "ƒë·ªãa ch·ªâ:"
        addr_text = soup.find(string=re.compile(r'ƒë·ªãa ch·ªâ:', re.IGNORECASE))
        if addr_text:
            # L·∫•y text sau "ƒë·ªãa ch·ªâ:"
            match = re.search(r'ƒë·ªãa ch·ªâ:\s*([^\n]+)', addr_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()[:300]
        
        # Pattern 3: T√¨m trong intro-in
        intro = soup.find('div', class_='intro-in')
        if intro:
            addr_p = intro.find('p', string=re.compile(r'ƒë·ªãa ch·ªâ:', re.IGNORECASE))
            if addr_p:
                text = addr_p.text.strip()
                match = re.search(r'ƒë·ªãa ch·ªâ:\s*([^\n]+)', text, re.IGNORECASE)
                if match:
                    return match.group(1).strip()[:300]
        
        return ""
    
    def parse_restaurant(self, url: str) -> Dict | None:
        """Parse th√¥ng tin 1 nh√† h√†ng"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content.decode('utf-8'), 'html.parser')
            
            # Extract title/name
            title_tag = soup.find('h1') or soup.find('title')
            name = title_tag.text.strip() if title_tag else ""
            
            # Extract description - PATTERN 1: T√¨m trong div#abouts (GI·ªöI THI·ªÜU)
            description = ""
            abouts = soup.find('div', id='abouts')
            if abouts:
                paragraphs = abouts.find_all('p')
                desc_parts = []
                for p in paragraphs[:3]:
                    text = p.text.strip()
                    # B·ªè qua c√°c paragraph qu√° ng·∫Øn ho·∫∑c ch·ªâ c√≥ s·ªë ƒëi·ªán tho·∫°i
                    if len(text) > 30 and not re.match(r'^\d+$', text):
                        desc_parts.append(text)
                description = ' '.join(desc_parts)[:800]
            
            # PATTERN 2: N·∫øu kh√¥ng c√≥, th·ª≠ t√¨m trong intro-in
            if not description:
                intro = soup.find('div', class_='intro-in')
                if intro:
                    paragraphs = intro.find_all('p')
                    desc_parts = []
                    for p in paragraphs[:3]:
                        text = p.text.strip()
                        if len(text) > 30 and not re.match(r'^\d+$', text):
                            desc_parts.append(text)
                    description = ' '.join(desc_parts)[:800]
            
            # PATTERN 3: T√¨m c√°c paragraph sau heading "GI·ªöI THI·ªÜU"
            if not description:
                intro_heading = soup.find(string=re.compile(r'gi·ªõi thi·ªáu', re.IGNORECASE))
                if intro_heading and intro_heading.parent:
                    parent = intro_heading.parent
                    desc_parts = []
                    # T√¨m c√°c paragraph ti·∫øp theo
                    for sibling in parent.find_next_siblings(limit=5):
                        if sibling.name == 'p':
                            text = sibling.text.strip()
                            if len(text) > 30:
                                desc_parts.append(text)
                    if desc_parts:
                        description = ' '.join(desc_parts[:3])[:800]
            
            # Extract metadata
            phone = self.extract_phone(soup)
            address = self.extract_address(soup)
            
            # Infer cuisine type from content
            content_text = soup.get_text().lower()
            cuisine_keywords = {
                'vi·ªát': ['vi·ªát nam', 'c∆°m', 'ph·ªü', 'b√∫n', 'm√≥n vi·ªát'],
                'nh·∫≠t': ['nh·∫≠t b·∫£n', 'sushi', 'ramen', 'izakaya', 'sake'],
                'h√†n': ['h√†n qu·ªëc', 'kimchi', 'bbq h√†n', 'korean'],
                '√¢u': ['√¢u', 'steak', 'pasta', 'pizza', 'italian', 'french'],
                'trung': ['trung hoa', 'dimsum', 'qu·∫£ng ƒë√¥ng', 'hongkong', 'dim sum']
            }
            
            cuisine_type = []
            for cuisine, keywords in cuisine_keywords.items():
                if any(kw in content_text for kw in keywords):
                    cuisine_type.append(cuisine)
            
            # Infer price range - C·∫¢I THI·ªÜN LOGIC
            price_range = "trung_binh"  # Default
            
            # T√¨m price text
            price_text = ""
            price_elements = soup.find_all(string=re.compile(r'(gi√°|price|vnƒë)', re.IGNORECASE))
            for elem in price_elements:
                text = elem.strip()
                if 10 < len(text) < 300 and any(word in text.lower() for word in ['gi√°', 'price', 'vnƒë', 'ƒë·ªìng']):
                    price_text += " " + text.lower()
            
            # Ph√¢n lo·∫°i d·ª±a tr√™n keywords v√† s·ªë ti·ªÅn
            if any(word in price_text for word in ['cao c·∫•p', 'sang tr·ªçng', 'ƒë·∫≥ng c·∫•p', 'luxury', 'premium']):
                price_range = "cao_cap"
            elif any(word in price_text for word in ['b√¨nh d√¢n', 'gi√° r·∫ª', 'ph·∫£i chƒÉng', 'affordable', 'budget']):
                price_range = "binh_dan"
            else:
                # D·ª±a tr√™n s·ªë ti·ªÅn
                numbers = re.findall(r'(\d{1,3}(?:[.,]\d{3})*)', price_text)
                if numbers:
                    try:
                        # L·∫•y s·ªë l·ªõn nh·∫•t
                        max_price = max([int(n.replace('.', '').replace(',', '')) for n in numbers])
                        if max_price > 500000:
                            price_range = "cao_cap"
                        elif max_price < 200000:
                            price_range = "binh_dan"
                    except:
                        pass
            
            # Extract features
            features = []
            feature_keywords = {
                'view_dep': ['view ƒë·∫πp', 't·∫ßm nh√¨n', 'panorama', 'rooftop'],
                'sang_trong': ['sang tr·ªçng', 'ƒë·∫≥ng c·∫•p', 'cao c·∫•p', 'luxury'],
                'am_cung': ['·∫•m c√∫ng', 'th√¢n thi·ªán', 'g·∫ßn g≈©i', 'cozy'],
                'hen_ho': ['h·∫πn h√≤', 'l√£ng m·∫°n', 't√¨nh nh√¢n', 'romantic'],
                'gia_dinh': ['gia ƒë√¨nh', 'tr·∫ª em', 'sum h·ªçp', 'family'],
                'cong_ty': ['c√¥ng ty', 'team building', 'ti·ªác', 's·ª± ki·ªán']
            }
            
            for feature, keywords in feature_keywords.items():
                if any(kw in content_text for kw in keywords):
                    features.append(feature)
            
            restaurant = {
                'url': url,
                'name': name,
                'description': description,
                'phone': phone,
                'address': address,
                'cuisine_type': cuisine_type,
                'price_range': price_range,
                'features': features,
                'full_content': content_text[:2000]  # For better embedding
            }
            
            return restaurant
            
        except Exception as e:
            print(f"‚ùå Error parsing {url}: {e}")
            return None
    
    def crawl_all(self, output_file: str = 'data/raw/restaurants.json', max_pages_per_category: int = 5, max_restaurants: int | None = None):
        """Crawl t·∫•t c·∫£ nh√† h√†ng
        
        Args:
            output_file: File ƒë·∫ßu ra
            max_pages_per_category: S·ªë trang t·ªëi ƒëa cho m·ªói category
            max_restaurants: S·ªë l∆∞·ª£ng nh√† h√†ng t·ªëi ƒëa c·∫ßn crawl (None = kh√¥ng gi·ªõi h·∫°n)
        """
        print("üîç Getting restaurant URLs...")
        urls = self.get_restaurant_urls(max_pages_per_category=max_pages_per_category)
        
        if max_restaurants:
            urls = urls[:max_restaurants]
            print(f"üìä Limiting to {len(urls)} restaurants")
        
        print(f"‚úÖ Will crawl {len(urls)} restaurants\n")
        
        restaurants = []
        for url in tqdm(urls, desc="üçΩÔ∏è  Crawling"):
            restaurant = self.parse_restaurant(url)
            if restaurant:
                restaurants.append(restaurant)
            time.sleep(0.5)  # Rate limiting
        
        # Save to JSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(restaurants, f, ensure_ascii=False, indent=2)
        
        print(f"\n‚úÖ Saved {len(restaurants)} restaurants to {output_file}")
        return restaurants

# Ch·∫°y crawler
if __name__ == "__main__":
    crawler = TopGoCrawler()
    restaurants = crawler.crawl_all()